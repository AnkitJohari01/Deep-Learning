{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srhBwr8YK3bK"
      },
      "source": [
        "## Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SiJCB-xZKj2z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQnB952MLFkT"
      },
      "source": [
        "<hr>\n",
        "\n",
        "# Step 1: Load and preprocess the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q--xCNr9LTKs"
      },
      "source": [
        "**Define paths for the training and validation datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDeIhggJM2AN",
        "outputId": "5d1a5f63-85c7-4b80-c4df-868b932dc374"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# List directories in /content to locate the dataset\n",
        "import os\n",
        "import zipfile\n",
        "from shutil import copy2\n",
        "from sklearn.model_selection import train_test_split\n",
        "os.listdir('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3AVt20-OUBY",
        "outputId": "a59f1459-7cab-48a3-cb44-679cd7e7d3b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: missing destination file operand after '/content/kaggle.json'\n",
            "Try 'cp --help' for more information.\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Dataset URL: https://www.kaggle.com/datasets/salader/dogs-vs-cats\n",
            "License(s): unknown\n",
            "Downloading dogs-vs-cats.zip to /content\n",
            "100% 1.06G/1.06G [00:08<00:00, 206MB/s]\n",
            "100% 1.06G/1.06G [00:08<00:00, 142MB/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json  # Update with the actual path to your kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d salader/dogs-vs-cats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Bo2cCxesLByx"
      },
      "outputs": [],
      "source": [
        "base_dir = '/content/dogs-vs-cats'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wHAcAalSPrrH"
      },
      "outputs": [],
      "source": [
        "# Create directories for training and validation sets\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "os.makedirs(os.path.join(validation_dir, 'cats'), exist_ok=True)\n",
        "os.makedirs(os.path.join(validation_dir, 'dogs'), exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c9PWLM5BNJ_0"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/dogs-vs-cats.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/dogs-vs-cats')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BXmV74J8Py0u"
      },
      "outputs": [],
      "source": [
        "# Split dataset\n",
        "for category in ['cats', 'dogs']:\n",
        "    category_dir = os.path.join(train_dir, category)\n",
        "    images = os.listdir(category_dir)\n",
        "    train_images, val_images = train_test_split(images, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Move validation images\n",
        "    for image in val_images:\n",
        "        src = os.path.join(category_dir, image)\n",
        "        dest = os.path.join(validation_dir, category, image)\n",
        "        copy2(src, dest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pkIyYFXL7Dh"
      },
      "source": [
        "**Initialize ImageDataGenerator with augmentation for the training set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5SFzq-ZKLzWK"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6t8a0dFMGVF"
      },
      "source": [
        "**No augmentation for the validation set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jYxhbXm_MDJs"
      },
      "outputs": [],
      "source": [
        "val_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yINbCT8jQCim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0900c5-9bd2-4b87-ceea-aa1a2074386a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX4mrREbMTWp"
      },
      "source": [
        "**Create training and validation generators**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tsGDX-IAMMNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93f6c7ff-49cc-424b-88fe-7a741dd5b73f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 images belonging to 2 classes.\n",
            "Found 4000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    validation_dir, ##val_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka0xEpSbQzkN"
      },
      "source": [
        "# Step 2: Load the VGG16 model without the top layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-D1Ae9rcMYou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "395c4c88-049c-4bac-9ba6-4abb2927e943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "base_model.trainable = False  # Freeze the convolutional base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9-9ZCN4Q9xm"
      },
      "source": [
        "# Step 3: Add custom layers for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "S6lgS3_yQ3bg"
      },
      "outputs": [],
      "source": [
        "x = Flatten()(base_model.output)  # Flatten the base model output\n",
        "x = Dense(256, activation='relu')(x)  # Add fully connected layer\n",
        "x = Dropout(0.5)(x)  # Dropout for regularization\n",
        "output = Dense(1, activation='sigmoid')(x)  # Output layer for binary classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeaXWn79RE8n"
      },
      "source": [
        "**Define the complete model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7uWm_6QkRABb"
      },
      "outputs": [],
      "source": [
        "model = Model(inputs=base_model.input, outputs=output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXScT8SmRMij"
      },
      "source": [
        "# Step 4: Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_Wn069rdRHv5"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLJ-AaCnRUQz"
      },
      "source": [
        "# Step 5: Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DgA75KLORPq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a78b3d-9693-4cf1-8fcc-dea0a5c28885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 252ms/step - accuracy: 0.7101 - loss: 0.5449 - val_accuracy: 0.8700 - val_loss: 0.2974\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 250ms/step - accuracy: 0.8167 - loss: 0.3962 - val_accuracy: 0.8785 - val_loss: 0.2862\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 249ms/step - accuracy: 0.8241 - loss: 0.3875 - val_accuracy: 0.8910 - val_loss: 0.2649\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 250ms/step - accuracy: 0.8352 - loss: 0.3612 - val_accuracy: 0.8808 - val_loss: 0.2698\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 248ms/step - accuracy: 0.8424 - loss: 0.3494 - val_accuracy: 0.8873 - val_loss: 0.2493\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=10,  # Adjust based on resources\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Evaluate the model"
      ],
      "metadata": {
        "id": "BMMcBwhzlA2Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ne3U24WVRWeG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4acffda-6d8c-438a-c80d-410586f636da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 74ms/step - accuracy: 0.8875 - loss: 0.2508\n",
            "Validation Accuracy: 0.89\n"
          ]
        }
      ],
      "source": [
        "val_loss, val_accuracy = model.evaluate(validation_generator)\n",
        "print(f\"Validation Accuracy: {val_accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Prediction function"
      ],
      "metadata": {
        "id": "NKY4J48QlSEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(150, 150))  # Load image\n",
        "    img_array = image.img_to_array(img) / 255.0  # Normalize image\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    prediction = model.predict(img_array)  # Predict the class\n",
        "    confidence = prediction[0][0] if prediction[0][0] >= 0.5 else 1 - prediction[0][0]\n",
        "    category = \"Dog\" if prediction[0][0] >= 0.5 else \"Cat\"\n",
        "    return category, confidence"
      ],
      "metadata": {
        "id": "1dRmU-7BlD9a"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type('/content/Dog.jpeg'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iDcjmljrN9e",
        "outputId": "8be68086-d881-406b-dc49-af8deb44e0cc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image"
      ],
      "metadata": {
        "id": "1p-pLCgXrqwX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image as keras_image"
      ],
      "metadata": {
        "id": "XIre-dcurv1P"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage for prediction\n",
        "category, confidence = predict_image('/content/Dog.jpeg')\n",
        "print(f\"Predicted Category: {category} with Confidence: {confidence:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPQfo2sZlVl2",
        "outputId": "3f89ec24-110f-4f2a-aa10-49f6672efbcf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Predicted Category: Dog with Confidence: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HsW-y3J3l5Dm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}